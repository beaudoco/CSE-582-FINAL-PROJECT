{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# START/END OF SENT TOKENS\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# MAX SENTENCE LEN\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# HIDDEN DIM OF RNN\n",
    "hidden_size = 256\n",
    "\n",
    "# LIST OF PHRASES WE WANT SENTENCES TO START WITH\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED TO DEFINE A LANGUAGE DICTIONARY W/ OTHER NECESSARY COMPONENTS\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # WORD INDEX REP, WORD COUNT, INDEX WORD REP\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        # EVERY LANG NEEDS START/END TOKEN\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        # CHECK IF WORD IN DICT ALREADY, IF NOT INSERT\n",
    "        # ELSE WE JUST UPDATE THE WORD COUNT\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse input/output order, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    # REMOVE PAIRS WHERE INPUT/OUTPUT TOO LONG AND ONLY USE\n",
    "    # THE START PHRASES WE WANTED \n",
    "    # print(p)\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH #and \\\n",
    "        # p[1].startswith(eng_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    # GET OUR LANGUAGE DICTS AND THE SENTENCES\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    # REMOVE TOO LONG SENTENCES, SENTENCES W/ INCORRECT START\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    # FILL OUR DICT\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 4004 sentence pairs\n",
      "Trimmed to 3785 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 1702\n",
      "sindarin 2260\n",
      "[0, 107, 445, 704, 614, 682, 556, 391, 179, 107, 0]\n",
      "[0, 285, 1036, 989, 671, 396, 196, 137, 57, 18, 0]\n",
      "['i want water', 'aniron nen']\n"
     ]
    }
   ],
   "source": [
    "# GET OUR INPUT/OUTPUT DICT AND THE USABLE SENTENCES\n",
    "# input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "input_lang, output_lang, pairs = prepareData('sindarin', 'eng', True)\n",
    "# PRINT RANDOM SENTENCE EXAMPLE\n",
    "# print(random.choice(pairs))\n",
    "\n",
    "random.shuffle(pairs)\n",
    "\n",
    "eng_arr_len = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "sind_arr_len = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "for pair in pairs:\n",
    "    eng_arr_len[len(pair[0].split())] += 1\n",
    "    sind_arr_len[len(pair[1].split())] += 1\n",
    "\n",
    "print(eng_arr_len)\n",
    "print(sind_arr_len)\n",
    "print(random.choice(pairs))\n",
    "\n",
    "train_pairs = pairs[:round((len(pairs) * .9))]\n",
    "test_pairs = pairs[round((len(pairs) * .9)):]\n",
    "pairs = train_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # EMBED OUR INPUT TO THE HIDDEN DIMENSION SPACE\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        # FEED OUR EMBEDDED INPUT TO AN RNN TO BUILD A \n",
    "        # SEQ REPRESENTATION OF THE EMBEDDED INPUT\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # USED TO INIT THE HIDDEN INPUT\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS JUST A STANDARD RNN DECODER EXAMPLE\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # EMBED OUR INPUT TO THE HIDDEN DIMENSION SPACE\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # ADD A NON-LINEARITY TO THE EMBEDDED INPUT\n",
    "        output = F.relu(output)\n",
    "        # FEED OUR EMBEDDED INPUT TO AN RNN TO BUILD A \n",
    "        # SEQ REPRESENTATION OF THE EMBEDDED INPUT\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # PREDICT OUR SENTENCE BASED ON THE END OUTPUT\n",
    "        # OF THE RNN\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # USED TO INIT THE HIDDEN INPUT\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # EMBED OUR INPUT TO THE HIDDEN DIMENSION SPACE\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # ADD SOME DROPOUT TO THE EMBEDDED SPACE TO PREVENT\n",
    "        # OVERFITTING THAT MAY OCCUR DURING TRAINING\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # COMBINE THE HIDDEN STATE AND THE INPUT AND LEARN SOME\n",
    "        # ATTN FOR THE INPUT, CONVERT TO WEIGHTS\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # APPLY WEIGHT TO ENCODER INFO SO WE FOCUS ON CORRECT\n",
    "        # AREA OF SENTENCE\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # COMBINE OUR ATTN EMBEDDING W/ EMBEDDED INPUT\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        # ADD A NON-LINEARITY TO THE ATTN SENTENCE\n",
    "        output = F.relu(output)\n",
    "        # FEED OUR ATTN SENTENCE TO AN RNN TO BUILD A \n",
    "        # SEQ REPRESENTATION OF THE OUTPUT\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        # PREDICT OUR SENTENCE BASED ON THE END OUTPUT\n",
    "        # OF THE RNN\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        # USED TO INIT THE HIDDEN INPUT\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE SENTENCE BASED ON DICT INDEX\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TENSOR OF SENTENCE TOKENS\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE INPUT AND OUTPUT TENSORS\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    # USE TO DECIDE IF WE TAKE OUR PREDICTION OR THE GROUND TRUTH\n",
    "    teacher_forcing_ratio = 0.25\n",
    "    # INIT HIDDEN STATE OF ENCODER\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    # SET TENSORS TO 0 FOR ENCODER/DECORDER\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # INPUT/OUTPUT SENTENCE LEN\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    # TENSOR TO HOLD ENCORDER REP OF SENTENCE\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # FOR EACH WORD OF ENCODER BUILD REP\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # ADD START TOK TO DECODER INPUT\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # USE THE LEARNED REP OF ENCODER FOR DECODER\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # SEE IF WE SHOULD USE GROUND TRUTH OR NOT\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # PREDICT EACH WORD OF SENTENCE BASED ON PREV INPUT TERM\n",
    "    # USE THE HIDDEN STATE OF ENCODER & THE REPRESENTATION \n",
    "    # OF EACH TERM FROM ENCODER\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    # UDPATE MODEL\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    # DEFINE OPTIMIZERS FOR ENCODER & DECODER\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    # GET RANDOM TRAINING SENTENCES\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    # NLL LOSS\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        # TRAIN ON SENTENCE\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_pairs[0][1]))\n",
    "\n",
    "pair = test_pairs[0]\n",
    "print(type(pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10, showExamples=True):\n",
    "    predictions_words = []\n",
    "    references_words = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i in range(n):\n",
    "        # pair = random.choice(pairs)\n",
    "        pair = test_pairs[i]\n",
    "        if showExamples:\n",
    "            print('>', pair[0])\n",
    "            print('=', pair[1])\n",
    "        references.append(pair[1])\n",
    "        references_words.append(pair[1].split())\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        predictions_words.append(output_words[:-1])\n",
    "        output_sentence = ' '.join(output_words[:-1])\n",
    "        predictions.append(output_sentence)\n",
    "        if showExamples:\n",
    "            print(output_words)\n",
    "            print('<', output_sentence)\n",
    "            print('')\n",
    "        \n",
    "\n",
    "    from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "    from evaluate import load\n",
    "    # references = references_words\n",
    "    candidates = predictions_words\n",
    "    cherrychen = SmoothingFunction()\n",
    "    bleu_4_sum_score = corpus_bleu(references_words, candidates, smoothing_function=cherrychen.method7)\n",
    "    bleu_3_sum_score = corpus_bleu(references_words, candidates, weights=(0.33, 0.33, 0.33, 0), smoothing_function=cherrychen.method7)\n",
    "    bleu_2_sum_score = corpus_bleu(references_words, candidates, weights=(0.5, 0.5, 0, 0), smoothing_function=cherrychen.method7)\n",
    "    bleu_1_score = corpus_bleu(references_words, candidates, weights=(1, 0, 0, 0), smoothing_function=cherrychen.method7)\n",
    "    bleu_2_score = corpus_bleu(references_words, candidates, weights=(0, 1, 0, 0), smoothing_function=cherrychen.method7)\n",
    "    bleu_3_score = corpus_bleu(references_words, candidates, weights=(0, 0, 1, 0), smoothing_function=cherrychen.method7)\n",
    "    bleu_4_score = corpus_bleu(references_words, candidates, weights=(0, 0, 0, 1), smoothing_function=cherrychen.method7)\n",
    "    \n",
    "    # BLEU SUM SCORES\n",
    "    print(\"BLEU SUM SCORES:\")\n",
    "    print(bleu_4_sum_score)\n",
    "    print(bleu_3_sum_score)\n",
    "    print(bleu_2_sum_score)\n",
    "    \n",
    "    # BLEU IND SCORES\n",
    "    print(\"BLEU IND SCORES:\")\n",
    "    print(bleu_4_score)\n",
    "    print(bleu_3_score)\n",
    "    print(bleu_2_score)\n",
    "    print(bleu_1_score)\n",
    "\n",
    "\n",
    "    wer = load(\"wer\")\n",
    "    wer_score = wer.compute(predictions=predictions, references=references)\n",
    "    print(\"WER SCORE:\")\n",
    "    print(wer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 35s (- 22m 12s) (5000 6%) 3.7931\n",
      "3m 14s (- 21m 2s) (10000 13%) 2.9493\n",
      "4m 56s (- 19m 44s) (15000 20%) 2.2029\n",
      "6m 40s (- 18m 20s) (20000 26%) 1.6652\n",
      "8m 25s (- 16m 50s) (25000 33%) 1.2103\n",
      "10m 10s (- 15m 15s) (30000 40%) 0.9465\n",
      "11m 56s (- 13m 39s) (35000 46%) 0.7247\n",
      "13m 43s (- 12m 0s) (40000 53%) 0.5591\n",
      "15m 31s (- 10m 21s) (45000 60%) 0.4671\n",
      "17m 18s (- 8m 39s) (50000 66%) 0.3985\n",
      "19m 5s (- 6m 56s) (55000 73%) 0.3430\n",
      "20m 51s (- 5m 12s) (60000 80%) 0.3031\n",
      "22m 39s (- 3m 29s) (65000 86%) 0.2835\n",
      "24m 25s (- 1m 44s) (70000 93%) 0.2527\n",
      "26m 13s (- 0m 0s) (75000 100%) 0.2496\n"
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> you mean me ?\n",
      "= ni thelig ?\n",
      "['ni', 'thelidh', '?', '<EOS>']\n",
      "< ni thelidh ?\n",
      "\n",
      "> trap !\n",
      "= gadas !\n",
      "['gadas', '!', '<EOS>']\n",
      "< gadas !\n",
      "\n",
      "> where do you go ?\n",
      "= na mhan menidh ?\n",
      "['na', 'van', 'menig', '?', '<EOS>']\n",
      "< na van menig ?\n",
      "\n",
      "> we will not\n",
      "= avomh\n",
      "['avof', '<EOS>']\n",
      "< avof\n",
      "\n",
      "> may your horse be swift\n",
      "= aen lagor i roch l n\n",
      "['no', 'lim', 'i', 'aran', 'no', 'n', '<EOS>']\n",
      "< no lim i aran no n\n",
      "\n",
      "> follow the stream one day and go south\n",
      "= aphado i hirion er arad a pado na charad\n",
      "['mae', 'ah', 'i', 'edain', 'a', 'ar', '<EOS>']\n",
      "< mae ah i edain a ar\n",
      "\n",
      "> i don t understand now everything\n",
      "=  u chenion hi bain\n",
      "['avon', 'aniron', 'athen', '<EOS>']\n",
      "< avon aniron athen\n",
      "\n",
      "> i promise\n",
      "= gweston\n",
      "['im', '<EOS>']\n",
      "< im\n",
      "\n",
      "> i m not feeling happy\n",
      "=  u vathon alu\n",
      "['avon', 'cared', '<EOS>']\n",
      "< avon cared\n",
      "\n",
      "> i am a merchant\n",
      "= ni vachor\n",
      "['ni', 'nunadan', '<EOS>']\n",
      "< ni nunadan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpb5867/anaconda3/envs/TransCons/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU SUM SCORES:\n",
      "0.0893981108674668\n",
      "0.1510855991601059\n",
      "0.24759060481766332\n",
      "BLEU IND SCORES:\n",
      "0.019512942920593954\n",
      "0.05339766550049111\n",
      "0.14658403492746838\n",
      "0.4181977090773145\n",
      "WER SCORE:\n",
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU SUM SCORES:\n",
      "0.07832875467128161\n",
      "0.13742066430219577\n",
      "0.23239602486815544\n",
      "BLEU IND SCORES:\n",
      "0.015351845629706175\n",
      "0.04540115510278304\n",
      "0.1345190972306758\n",
      "0.40148881078131676\n",
      "WER SCORE:\n",
      "0.562351072279587\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, n=len(test_pairs), showExamples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
    "# candidates = [['this', 'is', 'a', 'test']]\n",
    "# score = corpus_bleu(references, candidates)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "# wer = load(\"wer\")\n",
    "# wer_score = wer.compute(predictions=[\"this is a test\", \"this is a test\"], references=[\"this is a test\", \"this is test\"])\n",
    "# print(wer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_words, attentions = evaluate(\n",
    "#     encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "# plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluateAndShowAttention(\"elle est trop petit .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluateAndShowAttention(\"je ne crains pas de mourir .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransCons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
